# Ollama Notebook

Ask AI about your documents for free and locally!

This project was made for people who want to use RAG systems without uploading their documents to third-party services like Google.

It runs completely **locally** using Ollama and a local LLM.

---

## ‚ö†Ô∏è Warning

> üöß **This project is still under development.**
>
> It is not production-ready and may contain bugs.
>
> ‚ö° Depending on the size of the documents and the model being used, it can consume a significant amount of RAM and CPU.
>
> Use at your own risk.

---

## üß† How It Works

- You upload documents (TXT, PDF, etc.)
- The system generates embeddings
- It performs semantic (vector) search
- A local LLM (Phi-4 via Ollama) answers your questions using your documents as context

---

## üì¶ Requirements

Before running the project, make sure you have:

- [Ollama](https://ollama.com/)
- Phi-4 model installed
- Bun installed
- Rust installed (required by Tauri)

---

## üöÄ Setup Guide

### 1Ô∏è‚É£ Install Ollama

Download and install Ollama from:

https://ollama.com/

---

### 2Ô∏è‚É£ Pull the Phi-4 model

```bash
ollama pull phi4
````

---

### 3Ô∏è‚É£ Start Ollama server

```bash
ollama serve
```

Keep this running in a separate terminal.

---

### 4Ô∏è‚É£ Clone the repository

```bash
git clone https://github.com/diegorezm/llm-notebook.git
cd ollama-notebook
```

---

### 5Ô∏è‚É£ Install dependencies (using Bun)

```bash
bun install
```

---

### 6Ô∏è‚É£ Run the Tauri app in development mode

```bash
bun run tauri dev
```

---

## üõ† Tech Stack

* Rust
* Tauri
* Bun
* SolidJS
